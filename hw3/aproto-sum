#!/usr/bin/env python
import argparse
import sys
import models
import heapq
from collections import namedtuple
from numpy import logaddexp as logadd
#from grade import grad

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
parser.add_argument('-x', '--xnum', dest='x', default=None, type=int, help='num of the sentence to decode')
opts = parser.parse_args()

tm = models.TM(opts.tm, sys.maxint)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase')
hyp = namedtuple('hyp', 'tmprob, lmprob, states, translation, ancprob')
def coverage(sequence):
    # Generate a coverage for a sequence of indexes #
    # You can do things like:
    #   c1 | c2 to "add" coverages
    #   c1 & c2 will return 0 if c1 and c2 do NOT overlap
    #   c1 & c2 will be != 0 if c1 and c2 DO overlap
    return reduce(lambda x,y: x|y, map(lambda i: long(1) << i, sequence), 0)

def coverage2str(c, n, on='o', off='.'):
    # Generate a length-n string representation of coverage c #
    return '' if n==0 else (on if c&1==1 else off) + coverage2str(c>>1, n-1, on, off)

def grad(f, e): 
    f,e = tuple(f), tuple(e)
    sent_logprob = 0.0
    # compute p(e) under the LM
    lm_state = lm.begin()
    lm_logprob = 0.0
    for word in e + ("</s>",):
        (lm_state, word_logprob) = lm.score(lm_state, word)
        lm_logprob += word_logprob
    sent_logprob += lm_logprob

    # alignments[i] is a list of all the phrases in f that could have
    # generated phrases starting at position i in e
    alignments = [[] for _ in e]
    for fi in xrange(len(f)):
        for fj in xrange(fi+1,len(f)+1):
            if f[fi:fj] in tm:
                for phrase in tm[f[fi:fj]]:
                    ephrase = tuple(phrase.english.split())
                    for ei in xrange(len(e)+1-len(ephrase)):
                        ej = ei+len(ephrase)
                        if ephrase == e[ei:ej]:
                            alignments[ei].append((ej, phrase.logprob, fi, fj))

    # Compute sum of probability of all possible alignments by dynamic programming.
    # To do this, recursively compute the sum over all possible alignments for each
    # each pair of English prefix (indexed by ei) and French coverage (indexed by 
    # coverage v), working upwards from the base case (ei=0, v=0) [i.e. forward chaining]. 
    # The final sum is the one obtained for the pair (ei=len(e), v=range(len(f))
    chart = [{} for _ in e] + [{}]
    chart[0][0] = 0.0
    for ei, sums in enumerate(chart[:-1]):
        for v in sums:
            for ej, logprob, fi, fj in alignments[ei]:
                if coverage(xrange(fi,fj)) & v == 0:
                    new_v = coverage(xrange(fi,fj)) | v
                    if new_v in chart[ej]:
                        chart[ej][new_v] = logadd(chart[ej][new_v], sums[v]+logprob)
                    else:
                        chart[ej][new_v] = sums[v]+logprob
    goal = coverage(xrange(len(f)))  
    if goal in chart[len(e)]:
        sent_logprob += chart[len(e)][goal]
        return sent_logprob
    else: 
        return None

def join_hyp(hi, hj): # joinging hi-hj 
    logprob = hi.lmprob+hj.lmprob
    logprob -= hj.ancprob
    ancprob = hi.ancprob
    # stitch middle 

    lm_state = hi.states[1]
    lm_state, lm_logprob = lm.score(lm_state, hj.states[0][0])
    logprob += lm_logprob 
    # if left has only 1 
    if len(hi.translation.split())==1: 
        ancprob += lm_logprob
        lstate = hi.states[0] + (hj.states[0][0],)
    else: 
        lstate = hi.states[0]
    if len(hj.translation.split())==1: 
        rstate = lm_state
    else: 
        lm_state, lm_logprob = lm.score(lm_state, hj.states[0][1])
        logprob += lm_logprob 
        rstate = hj.states[1]
    states = (lstate, rstate)
    return hyp(hi.tmprob+hj.tmprob, logprob, states, hi.translation+' '+hj.translation, ancprob)

def stitch(h): 
    lm_state = lm.begin()
    logprob = h.lmprob
    for word in h.states[0]: 
        lm_state, word_logprob = lm.score(lm_state, word)
        logprob += word_logprob
    logprob += lm.end(h.states[1])
    logprob -= h.ancprob
    return hyp(h.tmprob, logprob, h.states, h.translation, 0.0)

grades = 0.0
for snum, f in enumerate(input_sents):
    if opts.x: 
        if snum != opts.x: 
            continue 
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    stackses = [[{} for j in xrange(i+1)] for i in xrange(len(f)+1) ]
    for d in xrange(1,len(f)+1): 
        for i in xrange(d, len(f)+1): 
            j = i-d
            if f[j:i] in tm: # if interval itself is in PT
                for phrase in tm[f[j:i]]:
                    logprob = 0.0
                    ancprob = 0.0
                    p = phrase.english.split()
                    lm_state, lm_logprob = lm.score((), p[0])
                    ancprob += lm_logprob
                    if len(p)>1: 
                        lm_state, lm_logprob = lm.score((), p[1])
                        ancprob += lm_logprob
                    if len(p)>2: 
                        for pword in p[3:]: 
                            lm_state, lm_logprob = lm.score((), pword)
                            logprob += lm_logprob
                    logprob += ancprob
                    states = (tuple(p[:2]), lm_state) # partial LM scores, ending states
                    if phrase.english not in stackses[i][j]: 
                        stackses[i][j][phrase.english] = hyp(phrase.logprob, logprob, states, phrase.english, ancprob)
                    else: 
                        tmprob = logadd(phrase.logprob, stackses[i][j][phrase.english].tmprob)
                        stackses[i][j][phrase.english] = hyp(tmprob, logprob, states, phrase.english, ancprob)

            for x in xrange(j+1,i): 
                for hi in stackses[i][x]: 
                    for hj in  stackses[x][j]:
                        #print (i, x), (x, j)
                        hyp_ = join_hyp(hi, hj)
                        if hyp_.translation in stackses[i][j]: 
                            tmprob = logadd(hyp_.tmprob,stackses[i][j][hyp_.translation].tmprob)
                            hyp_ = hyp(tmprob, hyp_.lmprob, hyp_.states, hyp_.translation, hyp_.ancprob)
                        stackses[i][j][hyp_.translation] = hyp_
                        hyp_ = join_hyp(hj, hi)
                        if hyp_.translation in stackses[i][j]: 
                            tmprob = logadd(hyp_.tmprob,stackses[i][j][hyp_.translation].tmprob)
                            hyp_ = hyp(tmprob, hyp_.lmprob, hyp_.states, hyp_.translation, hyp_.ancprob)
                        stackses[i][j][hyp_.translation] = hyp_
            stackses[i][j] = heapq.nlargest(opts.s, stackses[i][j].itervalues(), key=lambda h: h.tmprob+h.lmprob)
            #print len(stackses[i][j])
    hyps = [stitch(h) for h in stackses[-1][0]]
    winner = max(hyps, key=lambda h: grad(f,h.translation.split()))
    
    # find best translation by looking at the best scoring hypothesis
    # on the last stack
    if opts.x: 
        print opts.x, 
    print winner.translation
    grade = grad(f,winner.translation.split())
    sys.stderr.write('%d, TM=%f, LM=%f, total=%f, grade=%f\n' % (snum, winner.tmprob, winner.lmprob, winner.lmprob+winner.tmprob, grade))
    grades += grade
    #if opts.verbose:
    #    def extract_tm_logprob(h):
    #        return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
    #    tm_logprob = extract_tm_logprob(winner)
    #    sys.stderr.write('LM = %f, TM = %f, Total = %f\n' % 
    #        (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
sys.stderr.write( 'Oversall %f\n' % grades) 
