#!/usr/bin/env python
import argparse
import sys
import models
import heapq
from collections import namedtuple
from numpy import logaddexp as logadd
#from grade import grad

parser = argparse.ArgumentParser(description='Simple phrase based decoder.')
parser.add_argument('-i', '--input', dest='input', default='data/input', help='File containing sentences to translate (default=data/input)')
parser.add_argument('-t', '--translation-model', dest='tm', default='data/tm', help='File containing translation model (default=data/tm)')
parser.add_argument('-s', '--stack-size', dest='s', default=1, type=int, help='Maximum stack size (default=1)')
parser.add_argument('-n', '--num_sentences', dest='num_sents', default=sys.maxint, type=int, help='Number of sentences to decode (default=no limit)')
parser.add_argument('-l', '--language-model', dest='lm', default='data/lm', help='File containing ARPA-format language model (default=data/lm)')
parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,  help='Verbose mode (default=off)')
opts = parser.parse_args()

tm = models.TM(opts.tm, sys.maxint)
lm = models.LM(opts.lm)
sys.stderr.write('Decoding %s...\n' % (opts.input,))
input_sents = [tuple(line.strip().split()) for line in open(opts.input).readlines()[:opts.num_sents]]

# hypothesis = namedtuple('hypothesis', 'logprob, lm_state, predecessor, phrase')
hyp = namedtuple('hyp', 'logprob, states, prev, next, phrase')
def coverage(sequence):
    # Generate a coverage for a sequence of indexes #
    # You can do things like:
    #   c1 | c2 to "add" coverages
    #   c1 & c2 will return 0 if c1 and c2 do NOT overlap
    #   c1 & c2 will be != 0 if c1 and c2 DO overlap
    return reduce(lambda x,y: x|y, map(lambda i: long(1) << i, sequence), 0)

def coverage2str(c, n, on='o', off='.'):
    # Generate a length-n string representation of coverage c #
    return '' if n==0 else (on if c&1==1 else off) + coverage2str(c>>1, n-1, on, off)

def grad(f, e): 
    f,e = tuple(f), tuple(e)
    sent_logprob = 0.0
    # compute p(e) under the LM
    lm_state = lm.begin()
    lm_logprob = 0.0
    for word in e + ("</s>",):
        (lm_state, word_logprob) = lm.score(lm_state, word)
        lm_logprob += word_logprob
    sent_logprob += lm_logprob

    # alignments[i] is a list of all the phrases in f that could have
    # generated phrases starting at position i in e
    alignments = [[] for _ in e]
    for fi in xrange(len(f)):
        for fj in xrange(fi+1,len(f)+1):
            if f[fi:fj] in tm:
                for phrase in tm[f[fi:fj]]:
                    ephrase = tuple(phrase.english.split())
                    for ei in xrange(len(e)+1-len(ephrase)):
                        ej = ei+len(ephrase)
                        if ephrase == e[ei:ej]:
                            alignments[ei].append((ej, phrase.logprob, fi, fj))

    # Compute sum of probability of all possible alignments by dynamic programming.
    # To do this, recursively compute the sum over all possible alignments for each
    # each pair of English prefix (indexed by ei) and French coverage (indexed by 
    # coverage v), working upwards from the base case (ei=0, v=0) [i.e. forward chaining]. 
    # The final sum is the one obtained for the pair (ei=len(e), v=range(len(f))
    chart = [{} for _ in e] + [{}]
    chart[0][0] = 0.0
    for ei, sums in enumerate(chart[:-1]):
        for v in sums:
            for ej, logprob, fi, fj in alignments[ei]:
                if coverage(range(fi,fj)) & v == 0:
                    new_v = coverage(range(fi,fj)) | v
                    if new_v in chart[ej]:
                        chart[ej][new_v] = logadd(chart[ej][new_v], sums[v]+logprob)
                    else:
                        chart[ej][new_v] = sums[v]+logprob
    goal = coverage(range(len(f)))  
    if goal in chart[len(e)]:
        sent_logprob += chart[len(e)][goal]
        return sent_logprob
    else: 
        return None


def join_hyp(hi, hj): # joinging hi-hj 
    logprob = hi.logprob+hj.logprob
    # stitch middle 
    middle = hi.states[1] + hj.states[0]
    for i in xrange(2,len(middle)): 
        logprob += lm.score((middle[i-2],middle[i-1]), middle[i])[1]
    # stitch begin 
    rstate = hj.states[1] if len(hj.states[1])>1 else (hi.states[1][-1:],)+hj.states[1]
    lstate = hi.states[0] if len(hi.states[0])>1 else hi.states[0]+(hj.states[0][:1],)
    states = (lstate, rstate)
    return hyp(logprob, states, hi, hj, None)

def stitch(h): 
    lm_state = lm.begin()
    logprob = h.logprob
    for word in h.states[0]: 
        lm_state, word_logprob = lm.score(lm_state, word)
        logprob += word_logprob
    logprob += lm.end(h.states[1])
    return hyp(logprob, h.states, h.prev, h.next, h.phrase)
def extract_english_recursive(h):
    return (extract_english_recursive(h.prev) if h.prev else '') +\
        (h.phrase.english+' ' if h.phrase else '')+\
        (extract_english_recursive(h.next) if h.next else '')
grades = 0.0
for f in input_sents:
    # The following code implements a DP monotone decoding
    # algorithm (one that doesn't permute the target phrases).
    # Hence all hypotheses in stacks[i] represent translations of 
    # the first i words of the input sentence.
    # HINT: Generalize this so that stacks[i] contains translations
    # of any i words (remember to keep track of which words those
    # are, and to estimate future costs)
    stackses = [[{} for j in xrange(i+1)] for i in xrange(len(f)+1) ]
    for d in xrange(1,len(f)+1): 
        for i in xrange(d, len(f)+1): 
            j = i-d
            #print i, j, f[j:i]
            if f[j:i] in tm: # if interval itself is in PT
                for phrase in tm[f[j:i]]:
                    logprob = phrase.logprob
                    p = phrase.english.split()
                    lm_state = tuple(p[:2])
                    states = (tuple(p[:2]), tuple(p[-2:]))
                    for word in p[2:]: 
                        (lm_state, word_logprob) = lm.score(lm_state, word)
                        logprob += word_logprob
                    if states not in stackses[i][j] or stackses[i][j][states].logprob < logprob: 
                            stackses[i][j][states] = hyp(logprob, states, None, None, phrase)
                    # add to stackses[i][j]
            for x in xrange(j+1,i): 
                for hi in stackses[i][x]: 
                    for hj in  stackses[x][j]:
                        #print (i, x), (x, j)
                        hyp_ = join_hyp(hi, hj)
                        if hyp_.states not in stackses[i][j] or stackses[i][j][hyp_.states].logprob < logprob: 
                            stackses[i][j][hyp_.states] = hyp_
                        hyp_ = join_hyp(hj, hi)
                        if hyp_.states not in stackses[i][j] or stackses[i][j][hyp_.states].logprob < logprob: 
                            stackses[i][j][hyp_.states] = hyp_
            stackses[i][j] = heapq.nlargest(opts.s, stackses[i][j].itervalues(), key=lambda h: h.logprob)
            #print len(stackses[i][j])
    for h in stackses[-1][0]: 
        stitch(h)
    hyps = [stitch(h) for h in stackses[-1][0]]
    winner = max(hyps, key=lambda h: grad(f,extract_english_recursive(h).split()))
    
    # find best translation by looking at the best scoring hypothesis
    # on the last stack

    print extract_english_recursive(winner)
    grade = grad(f,extract_english_recursive(winner).split())
    sys.stderr.write('LM+TM = %f, grade=%f\n' % (winner.logprob, grade))
    grades += grade
    #if opts.verbose:
    #    def extract_tm_logprob(h):
    #        return 0.0 if h.predecessor is None else h.phrase.logprob + extract_tm_logprob(h.predecessor)
    #    tm_logprob = extract_tm_logprob(winner)
    #    sys.stderr.write('LM = %f, TM = %f, Total = %f\n' % 
    #        (winner.logprob - tm_logprob, tm_logprob, winner.logprob))
sys.stderr.write( 'Oversall%f\n' % grades) 
